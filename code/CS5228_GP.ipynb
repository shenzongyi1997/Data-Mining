{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# visualiation\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# model evaluation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier # KNN\n",
    "from sklearn.linear_model import LogisticRegression # logistic regression\n",
    "from sklearn.tree import DecisionTreeClassifier # decision tree\n",
    "from sklearn.ensemble import RandomForestClassifier # random forest\n",
    "from sklearn.ensemble import GradientBoostingClassifier # gradient boosting\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn import svm\n",
    "import category_encoders as ce\n",
    "ori_demo = pd.read_csv(\"../data/train.csv\")\n",
    "ori_test_input = pd.read_csv(\"../data/test.csv\")\n",
    "# test_data = pd.read_csv(\"../data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the dataframe: (24421, 14)\n",
      "category feature: ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'sex', 'native-country']\n",
      "continuous feature: ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n"
     ]
    }
   ],
   "source": [
    "# preparation to preprocess\n",
    "print(\"shape of the dataframe:\",ori_demo.shape)\n",
    "category_features = []\n",
    "continuous_features = []\n",
    "for (column,dtype) in zip((ori_demo.drop(columns=\"exceeds50K\")).columns, (ori_demo.drop(columns=\"exceeds50K\")).dtypes):\n",
    "    if dtype==\"int64\":\n",
    "        continuous_features.append(column)\n",
    "    else:\n",
    "        category_features.append(column)\n",
    "print(\"category feature:\", category_features)\n",
    "print(\"continuous feature:\",continuous_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values print\n",
    "# deal ? as a new value\n",
    "print('Before replacing missing values: ')\n",
    "for column in ori_demo.columns:\n",
    "    missing_values = pd.isna(ori_demo[column]).sum()\n",
    "    print(column,\" has \",missing_values,\" missing values\")\n",
    "    \n",
    "# deal ? as missing value    \n",
    "for column in category_features:\n",
    "    cates = ori_demo[column].unique().tolist()\n",
    "    for cate in cates:\n",
    "        if \"?\" in cate:\n",
    "            print(column, \" has missing values!\")\n",
    "            print(ori_demo[column].value_counts())\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "outcomes = ori_demo[\"exceeds50K\"]\n",
    "print(ori_demo[\"exceeds50K\"].value_counts())\n",
    "sb.countplot(x=\"exceeds50K\",data=ori_demo).set_title(\"number of different outputs\")\n",
    "nums_of_cates = []\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.title(\"unique values of category attributes\")\n",
    "for column in category_features:\n",
    "    nums_of_cates.append(len(ori_demo[column].value_counts().tolist()))\n",
    "plt.bar(category_features, nums_of_cates)\n",
    "print(nums_of_cates)\n",
    "mins_of_conts = []\n",
    "maxs_of_conts = []\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"scale of continuous attribtues\")\n",
    "for column in continuous_features:\n",
    "    mins_of_conts.append(ori_demo[column].min())\n",
    "    maxs_of_conts.append(ori_demo[column].max())\n",
    "plt.bar(continuous_features,maxs_of_conts,bottom=mins_of_conts,edgecolor = 'white')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "scales = []\n",
    "for i in range(len(continuous_features)):\n",
    "    scales.append(maxs_of_conts[i] - mins_of_conts[i])\n",
    "print(scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# preprocess with missing values(replace ? using most common value)\n",
    "\n",
    "\n",
    "# operation\n",
    "ori_demo = ori_demo.fillna(ori_demo.mean())\n",
    "# print('\\nAfter replacing missing values: ')\n",
    "# for column, mean_value in zip(ori_demo.columns, ori_demo.mean()):\n",
    "#     missing_values = pd.isna(ori_demo[column]).sum()\n",
    "for column in category_features:\n",
    "    mean_value = ori_demo[column].value_counts().index[0]\n",
    "#     print(\"mean\", mean_value)\n",
    "    ori_demo[column] = ori_demo[column].replace(regex=r'^.*\\?.*$',value=mean_value)\n",
    "#     print(column, ori_demo[column].unique())\n",
    "def basic_preprocess(data):\n",
    "    for column in category_features:\n",
    "        mean_value = data[column].value_counts().index[0]\n",
    "#         print(\"mean\", mean_value)\n",
    "        data[column] = data[column].replace(regex=r'^.*\\?.*$',value=mean_value)\n",
    "        print(\"column name:\",column,\", all values:\", data[column].unique())\n",
    "    return data\n",
    "ori_test_input = basic_preprocess(ori_test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0,
     1,
     12
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Class count and resampling\n",
    "def over_sampling(train_data, target_label):\n",
    "    count_0, count_1 = train_data[target_label].value_counts()\n",
    "    # Divide by class\n",
    "    train_data_0 = train_data[train_data[target_label] == 0]\n",
    "    train_data_1 = train_data[train_data[target_label] == 1]\n",
    "\n",
    "    train_data_1_over = train_data_1.sample(count_0, replace=True)\n",
    "    train_data_over = pd.concat([train_data_0, train_data_1_over], axis=0)\n",
    "    return train_data_over\n",
    "\n",
    "\n",
    "def under_sampling(train_data, target_label):\n",
    "    count_0, count_1 = train_data[target_label].value_counts()\n",
    "    # Divide by class\n",
    "    train_data_0 = train_data[train_data[target_label] == 0]\n",
    "    train_data_1 = train_data[train_data[target_label] == 1]\n",
    "\n",
    "    train_data_0_under = train_data_0.sample(count_1, replace=True)\n",
    "    train_data_under = pd.concat([train_data_0_under, train_data_1], axis=0)\n",
    "    return train_data_under\n",
    "\n",
    "\n",
    "ori_demo_over = over_sampling(ori_demo, \"exceeds50K\")\n",
    "ori_demo_under = under_sampling(ori_demo,\"exceeds50K\")\n",
    "\n",
    "# demo_over[\"exceeds50K\"].value_counts().plot(kind='bar', title='Count (output)');\n",
    "# demo_under[\"exceeds50K\"].value_counts().plot(kind='bar', title='Count (output)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# results over train set\n",
    "def test_clf_on_train(clf, train, output_label):\n",
    "    x = train.drop(columns=output_label)\n",
    "    y = train[output_label]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state=10)\n",
    "    clf.fit(x_train,y_train)\n",
    "    y_pred = clf.predict(x_test) \n",
    "    f1 = round(f1_score(y_test, y_pred, average='weighted') * 100, 2)\n",
    "    acc = round(accuracy_score(y_test, y_pred) * 100, 2)\n",
    "    return f1, acc\n",
    "\n",
    "\n",
    "def show_result(data, Need_SVM=False):   \n",
    "    if Need_SVM:\n",
    "        model_names = ['KNN', 'LR', 'DT', 'RF', 'GBM','GaussianNB','SVM'] \n",
    "    else:\n",
    "        model_names = ['KNN', 'LR', 'DT', 'RF', 'GBM','GaussianNB']\n",
    "    all_acc = []\n",
    "    all_f1 = []\n",
    "    # 1. KNN\n",
    "    clf_knn = KNeighborsClassifier()\n",
    "    acc, f1 = test_clf_on_train(clf_knn, data, data.columns[-1])\n",
    "    all_acc.append(acc)\n",
    "    all_f1.append(f1)\n",
    "\n",
    "    # 2. logistic regression\n",
    "    clf_LR = LogisticRegression()\n",
    "    acc, f1 = test_clf_on_train(clf_LR, data, data.columns[-1])\n",
    "    all_acc.append(acc)\n",
    "    all_f1.append(f1)\n",
    "\n",
    "    # 3. DecisionTree\n",
    "    clf_DT = DecisionTreeClassifier()\n",
    "    acc, f1 = test_clf_on_train(clf_DT, data, data.columns[-1])\n",
    "    all_acc.append(acc)\n",
    "    all_f1.append(f1)\n",
    "\n",
    "    # 4. RandomForest\n",
    "    clf_RF = RandomForestClassifier()\n",
    "    acc, f1 = test_clf_on_train(clf_RF, data, data.columns[-1])\n",
    "    all_acc.append(acc)\n",
    "    all_f1.append(f1)\n",
    "\n",
    "    # 5. Gradient Boosting\n",
    "    clf_GB = GradientBoostingClassifier()\n",
    "    f1, acc = test_clf_on_train(clf_GB, data, data.columns[-1])\n",
    "    all_acc.append(acc)\n",
    "    all_f1.append(f1)\n",
    "    \n",
    "    # 6. Gaussian Naive Bayes\n",
    "    clf_GussianNB = GaussianNB()\n",
    "    f1, acc = test_clf_on_train(clf_GussianNB, data, data.columns[-1])\n",
    "    all_acc.append(acc)\n",
    "    all_f1.append(f1)\n",
    "    \n",
    "    # 7. SVM # SVM\n",
    "    if Need_SVM:  \n",
    "        svclassifier = svm.SVC(kernel='linear')\n",
    "        acc,f1 = test_clf_on_train(svclassifier, data, data.columns[-1])\n",
    "        all_acc.append(acc)\n",
    "        all_f1.append(f1)\n",
    "    \n",
    "    results = pd.DataFrame({'Model': model_names, 'acc': all_acc, 'f1': all_f1})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# simple onehot preprocess\n",
    "def train_data_simple_preprocess(train, output_label=\"\"):\n",
    "#     ori_train = train.copy()\n",
    "    train = pd.get_dummies(train)\n",
    "    train_input = train\n",
    "    scalar = StandardScaler()\n",
    "    scalar.fit(train.drop(columns=output_label))\n",
    "    train_input = scalar.transform(train.drop(columns=output_label))\n",
    "    \n",
    "    new_train = pd.DataFrame(data=train_input, columns=train.drop(columns=output_label).columns)\n",
    "    new_train[output_label] = train[output_label]\n",
    "    return train_input, train[output_label], new_train\n",
    "\n",
    "print(ori_demo.shape)\n",
    "demo_input, demo_output, demo = train_data_simple_preprocess(ori_demo, \"exceeds50K\")\n",
    "print(demo.shape)\n",
    "# print&save onthot result\n",
    "demo_over = over_sampling(demo, \"exceeds50K\")\n",
    "demo_under = under_sampling(demo,\"exceeds50K\")\n",
    "result = show_result(demo)\n",
    "print(\"original:\")\n",
    "print(result)\n",
    "result_over = show_result(demo_over)\n",
    "print(\"original over:\")\n",
    "print(result_over)\n",
    "result_under = show_result(demo_under)\n",
    "print(\"original under:\")\n",
    "print(result_under)\n",
    "result.to_csv(\"result_simple_onehot.csv\")\n",
    "result_over.to_csv(\"result_simple_onehot_over.csv\")\n",
    "result_under.to_csv(\"result_simple_onehot_under.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24421, 14)\n",
      "(24421, 103)\n",
      "original:\n",
      "        Model    acc     f1\n",
      "0         KNN  74.62  77.22\n",
      "1          LR  75.97  79.50\n",
      "2          DT  81.34  81.31\n",
      "3          RF  84.40  84.77\n",
      "4         GBM  86.54  85.94\n",
      "5  GaussianNB  79.17  76.32\n",
      "original over:\n",
      "        Model    acc     f1\n",
      "0         KNN  72.77  72.93\n",
      "1          LR  60.90  61.34\n",
      "2          DT  91.13  91.15\n",
      "3          RF  92.84  92.86\n",
      "4         GBM  84.42  84.40\n",
      "5  GaussianNB  62.65  58.60\n",
      "original under:\n",
      "        Model    acc     f1\n",
      "0         KNN  61.87  61.89\n",
      "1          LR  58.51  62.37\n",
      "2          DT  80.10  80.11\n",
      "3          RF  84.31  84.31\n",
      "4         GBM  84.65  84.64\n",
      "5  GaussianNB  63.25  59.24\n"
     ]
    }
   ],
   "source": [
    "# simple onehot preprocess without normalization\n",
    "def train_data_simple_without_normalization_preprocess(train, output_label=\"\"):\n",
    "#     ori_train = train.copy()\n",
    "    train = pd.get_dummies(train)\n",
    "    train_input = train\n",
    "#     scalar = StandardScaler()\n",
    "#     scalar.fit(train.drop(columns=output_label))\n",
    "#     train_input = scalar.transform(train.drop(columns=output_label))\n",
    "    \n",
    "    new_train = pd.DataFrame(data=train_input, columns=train.drop(columns=output_label).columns)\n",
    "    new_train[output_label] = train[output_label]\n",
    "    return train_input, train[output_label], new_train\n",
    "\n",
    "print(ori_demo.shape)\n",
    "demo_input, demo_output, demo = train_data_simple_without_normalization_preprocess(ori_demo, \"exceeds50K\")\n",
    "print(demo.shape)\n",
    "# print&save onthot result\n",
    "demo_over = over_sampling(demo, \"exceeds50K\")\n",
    "demo_under = under_sampling(demo,\"exceeds50K\")\n",
    "result = show_result(demo)\n",
    "print(\"original:\")\n",
    "print(result)\n",
    "result_over = show_result(demo_over)\n",
    "print(\"original over:\")\n",
    "print(result_over)\n",
    "result_under = show_result(demo_under)\n",
    "print(\"original under:\")\n",
    "print(result_under)\n",
    "result.to_csv(\"result_simple_onehot.csv\")\n",
    "result_over.to_csv(\"result_simple_onehot_over.csv\")\n",
    "result_under.to_csv(\"result_simple_onehot_under.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hash category preprocess\n",
    "print(ori_demo.shape)\n",
    "def train_data_with_hash_cate_preprocess(train, output_label=\"\", threshold = 16, target_component = 16):\n",
    "    ce_hash = ce.HashingEncoder()\n",
    "    new_train = train.copy()\n",
    "    new_train = new_train.drop(columns=output_label)\n",
    "    for (column,dtype) in zip((train.drop(columns=output_label)).columns, (train.drop(columns=output_label)).dtypes):\n",
    "#         print(\"column:\",column)\n",
    "        if dtype == \"int64\":\n",
    "            continue\n",
    "        if train[column].unique().shape[0] > threshold:\n",
    "            temp_df = ce_hash.hashing_trick(train.loc[:, [column]], N=target_component)\n",
    "            for i, temp_col in zip(range(target_component), temp_df.columns):\n",
    "                new_train[column + str(i)] = temp_df[temp_col]\n",
    "            new_train = new_train.drop(columns = column)\n",
    "    new_train = pd.get_dummies(new_train)\n",
    "    scalar = StandardScaler()\n",
    "    scalar.fit(new_train)\n",
    "    train_input = scalar.transform(new_train)\n",
    "#     new_train = pd.DataFrame(data=train_input, columns=train.drop(columns=output_label).columns)\n",
    "    new_train[output_label] = train[output_label]\n",
    "    return train_input, train[output_label], new_train\n",
    "\n",
    "demo_hash_input, demo_hash_output, demo_hash = train_data_with_hash_cate_preprocess(ori_demo, output_label = \"exceeds50K\")\n",
    "demo_hash_over =  over_sampling(demo_hash, \"exceeds50K\")\n",
    "demo_hash_under = under_sampling(demo_under, \"exceeds50K\")\n",
    "# print&save hash result\n",
    "result_hash = show_result(demo_hash)\n",
    "print(\"hash:\")\n",
    "print(result_hash)\n",
    "result_hash_over = show_result(demo_hash_over)\n",
    "print(\"hash over:\")\n",
    "print(result_hash_over)\n",
    "result_hash_under = show_result(demo_hash_under)\n",
    "print(\"hash under:\")\n",
    "print(result_hash_under)\n",
    "result_hash.to_csv(\"result_hash16.csv\")\n",
    "result_hash_over.to_csv(\"result_hash16_over.csv\")\n",
    "result_hash_under.to_csv(\"results_hash16_under.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# target encode category preprocess\n",
    "# target category, and normalize continuous\n",
    "\n",
    "print(ori_demo.shape)\n",
    "def train_data_with_target_cate_preprocess(train, output_label=\"\"):\n",
    "    ce_target = ce.TargetEncoder()\n",
    "    new_train = train.copy()\n",
    "    new_train = new_train.drop(columns=output_label)\n",
    "    scalar = StandardScaler()\n",
    "    continuous_train = train[continuous_features]\n",
    "    scalar.fit(continuous_train)\n",
    "    new_continuous_train = scalar.transform(continuous_train)\n",
    "    category_train = train[category_features]\n",
    "    for column in category_train.columns:\n",
    "        ce_target.fit(category_train[column].values, train[output_label].values)\n",
    "        temp_df = ce_target.transform(train[column].values, train[output_label].values)\n",
    "        new_train[column] = temp_df\n",
    "    new_train[continuous_features] = new_continuous_train\n",
    "    new_train[output_label] = train[output_label]\n",
    "    train_input = new_train.drop(columns=output_label)\n",
    "    return train_input, train[output_label], new_train\n",
    "\n",
    "demo_target_input, demo_target_output, demo_target = train_data_with_target_cate_preprocess(ori_demo, output_label = \"exceeds50K\")\n",
    "# # --- End of your code ---\n",
    "print(demo_target_input.shape)\n",
    "demo_target.head()\n",
    "\n",
    "# target show result\n",
    "demo_target_over = over_sampling(demo_target,\"exceeds50K\")\n",
    "demo_target_under = under_sampling(demo_target,\"exceeds50K\")\n",
    "result_target = show_result(demo_target)\n",
    "print(\"target encoder:\")\n",
    "print(result_target)\n",
    "result_target_over = show_result(demo_target_over)\n",
    "print(\"target encoder over:\")\n",
    "print(result_target_over)\n",
    "result_target_under = show_result(demo_target_under)\n",
    "print(\"target encoder under:\")\n",
    "print(result_target_under)\n",
    "result_target.to_csv(\"result_target_encode.csv\")\n",
    "result_target_over.to_csv(\"result_target_encode_over.csv\")\n",
    "result_target_under.to_csv(\"result_target_encode_under.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# leave one out encode category preprocess\n",
    "# leave one out category, and normalize continuous\n",
    "\n",
    "print(ori_demo.shape)\n",
    "def train_data_with_leaveoneout_cate_preprocess(train, output_label=\"\"):\n",
    "    ce_leave = ce.LeaveOneOutEncoder()     \n",
    "    # Must pass the series for y\n",
    "    new_train = train.copy()\n",
    "    new_train = new_train.drop(columns=output_label)\n",
    "    scalar = StandardScaler()\n",
    "    continuous_train = train[continuous_features]\n",
    "    scalar.fit(continuous_train)\n",
    "    new_continuous_train = scalar.transform(continuous_train)\n",
    "    category_train = train[category_features]\n",
    "    for column in category_train.columns:\n",
    "        ce_leave.fit(category_train[column].values, train[output_label].values)\n",
    "        temp_df = ce_leave.transform(train[column].values, train[output_label].values)\n",
    "        new_train[column] = temp_df\n",
    "    new_train[continuous_features] = new_continuous_train\n",
    "    new_train[output_label] = train[output_label]\n",
    "    train_input = new_train.drop(columns=output_label)\n",
    "    return train_input, train[output_label], new_train\n",
    "\n",
    "demo_leave_input, demo_leave_output, demo_leave = train_data_with_leaveoneout_cate_preprocess(ori_demo, output_label = \"exceeds50K\")\n",
    "# # --- End of your code ---\n",
    "print(demo_leave_input.shape)\n",
    "demo_leave.head()\n",
    "\n",
    "# print leaveoneout result\n",
    "demo_leave_over = over_sampling(demo_leave, \"exceeds50K\")\n",
    "demo_leave_under = under_sampling(demo_leave, \"exceeds50K\")\n",
    "result_leave = show_result(demo_leave)\n",
    "print(\"leavebutone:\")\n",
    "print(result_leave)\n",
    "result_leave_over = show_result(demo_leave_over)\n",
    "print(\"leavebutone over:\")\n",
    "print(result_leave_over)\n",
    "result_leave_under = show_result(demo_leave_under)\n",
    "print(\"leavebutone under:\")\n",
    "print(result_leave_under)\n",
    "result_leave.to_csv(\"result_leave_encode.csv\")\n",
    "result_leave_over.to_csv(\"result_leave_encode_over.csv\")\n",
    "result_leave_under.to_csv(\"result_leave_encode_under.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# smooth target  encode category preprocess\n",
    "# target category, and normalize continuous\n",
    "\n",
    "print(ori_demo.shape)\n",
    "def train_data_with_target_cate_preprocess(train, output_label=\"\", smooth = 10):\n",
    "    ce_target = ce.TargetEncoder(smoothing = smooth)\n",
    "    new_train = train.copy()\n",
    "    new_train = new_train.drop(columns=output_label)\n",
    "    scalar = StandardScaler()\n",
    "    continuous_train = train[continuous_features]\n",
    "    scalar.fit(continuous_train)\n",
    "    new_continuous_train = scalar.transform(continuous_train)\n",
    "    category_train = train[category_features]\n",
    "    for column in category_train.columns:\n",
    "        ce_target.fit(category_train[column].values, train[output_label].values)\n",
    "        temp_df = ce_target.transform(train[column].values, train[output_label].values)\n",
    "        new_train[column] = temp_df\n",
    "    new_train[continuous_features] = new_continuous_train\n",
    "    new_train[output_label] = train[output_label]\n",
    "    train_input = new_train.drop(columns=output_label)\n",
    "    return train_input, train[output_label], new_train\n",
    "\n",
    "demo_sm_target_input, demo_sm_target_output, demo_sm_target = train_data_with_target_cate_preprocess(ori_demo, output_label = \"exceeds50K\")\n",
    "print(demo_sm_target_input.shape)\n",
    "demo_sm_target.head()\n",
    "\n",
    "# print smooth_target result\n",
    "demo_sm_target_over = over_sampling(demo_sm_target,\"exceeds50K\")\n",
    "demo_sm_target_under = under_sampling(demo_sm_target,\"exceeds50K\")\n",
    "result_sm_target = show_result(demo_sm_target)\n",
    "print(\"smooth target encoder:\")\n",
    "print(result_sm_target)\n",
    "result_sm_target_over = show_result(demo_sm_target_over)\n",
    "print(\"smooth target encoder over:\")\n",
    "print(result_sm_target_over)\n",
    "result_sm_target_under = show_result(demo_sm_target_under)\n",
    "print(\"smooth target encoder under:\")\n",
    "print(result_sm_target_under)\n",
    "result_sm_target.to_csv(\"result_smooth_target.csv\")\n",
    "result_sm_target_over.to_csv(\"result_smooth_target_over.csv\")\n",
    "result_sm_target_under.to_csv(\"result_smooth_target_under.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#james_stein\n",
    "print(ori_demo.shape)\n",
    "def train_data_with_james_cate_preprocess(train, output_label=\"\", smooth = 10):\n",
    "    ce_james = ce.james_stein.JamesSteinEncoder()\n",
    "    new_train = train.copy()\n",
    "    new_train = new_train.drop(columns=output_label)\n",
    "    scalar = StandardScaler()\n",
    "    continuous_train = train[continuous_features]\n",
    "    scalar.fit(continuous_train)\n",
    "    new_continuous_train = scalar.transform(continuous_train)\n",
    "    category_train = train[category_features]\n",
    "    for column in category_train.columns:\n",
    "        ce_james.fit(category_train[column].values, train[output_label].values)\n",
    "        temp_df = ce_james.transform(train[column].values, train[output_label].values)\n",
    "        new_train[column] = temp_df\n",
    "    new_train[continuous_features] = new_continuous_train\n",
    "    new_train[output_label] = train[output_label]\n",
    "    train_input = new_train.drop(columns=output_label)\n",
    "    return train_input, train[output_label], new_train\n",
    "\n",
    "demo_james_input, demo_james_output, demo_james = train_data_with_james_cate_preprocess(ori_demo, output_label = \"exceeds50K\")\n",
    "print(demo_james_input.shape)\n",
    "demo_james.head()\n",
    "\n",
    "# print ames result\n",
    "demo_james_over = over_sampling(demo_james,\"exceeds50K\")\n",
    "demo_james_under = under_sampling(demo_james,\"exceeds50K\")\n",
    "result_james = show_result(demo_james)\n",
    "print(\"james encoder:\")\n",
    "print(result_james)\n",
    "result_james_over = show_result(demo_james_over)\n",
    "print(\"james encoder over:\")\n",
    "print(result_james_over)\n",
    "result_james_under = show_result(demo_james_under)\n",
    "print(\"james encoder under:\")\n",
    "print(result_james_under)\n",
    "result_james.to_csv(\"result_james.csv\")\n",
    "result_james_over.to_csv(\"result_james_over.csv\")\n",
    "result_james_under.to_csv(\"result_james_under.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [
     9,
     11,
     13,
     28,
     36,
     55,
     71,
     87
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# deal with new catogory and test data preprocess\n",
    "def preprocess_new_catogory(train, test, mode = 0):\n",
    "    # must ensure train and test have same columns\n",
    "    # mode = 0, use the most often value\n",
    "    # mode = 1, use arbitrary value\n",
    "    i = 0\n",
    "    for column1, dtype, column2 in zip(train.columns, train.dtypes, test.columns):\n",
    "        if column1!=column2:\n",
    "            continue\n",
    "        if dtype == \"int64\":\n",
    "            continue\n",
    "#             print(\"start!\")\n",
    "        catos = train[column1].unique()\n",
    "#         print(\"column\",column,\"test[column]\",test[column])\n",
    "        temp = test[column2].apply(lambda x: False if x in catos else True)\n",
    "#         print(temp)\n",
    "        if test[temp].shape[0]>0:\n",
    "            print(column1, column2, test[temp].shape[0],\" needs change\")\n",
    "            candi = test.loc[temp, column2]\n",
    "            print(candi, \" not included\")\n",
    "            if mode == 0:\n",
    "                counts = test[column2].value_counts().index.tolist()\n",
    "                for count in counts:\n",
    "                    if count in catos:\n",
    "                        test.loc[temp, column2]=count\n",
    "                        break\n",
    "        i += 1\n",
    "    print(i,\" columns have been detected!\")\n",
    "    return test\n",
    "\n",
    "def test_data_simple_preprocess(test):\n",
    "    test = pd.get_dummies(test)\n",
    "    scalar = StandardScaler()\n",
    "    scalar.fit(test)\n",
    "    test_input = scalar.transform(test)\n",
    "    new_test = pd.DataFrame(data=test_input, columns=test.columns)\n",
    "    return new_test\n",
    "\n",
    "def test_data_simple_without_normalization_preprocess(test):\n",
    "    test_input = pd.get_dummies(test)\n",
    "#     scalar = StandardScaler()\n",
    "#     scalar.fit(test)\n",
    "#     test_input = scalar.transform(test)\n",
    "    new_test = pd.DataFrame(data=test_input, columns=test_input.columns)\n",
    "    return new_test\n",
    "\n",
    "def test_data_with_hash_cate_preprocess(test, threshold = 8, target_component = 8):\n",
    "    ce_hash = ce.HashingEncoder()\n",
    "    new_test = test.copy()\n",
    "    for (column,dtype) in zip(test.columns, test.dtypes):\n",
    "        print(\"column:\",column)\n",
    "        if dtype == \"int64\":\n",
    "            continue\n",
    "        if test[column].unique().shape[0] > threshold:\n",
    "            temp_df = ce_hash.hashing_trick(test.loc[:, [column]], N=target_component)\n",
    "            for i, temp_col in zip(range(target_component), temp_df.columns):\n",
    "                new_test[column + str(i)] = temp_df[temp_col]\n",
    "            new_test = new_test.drop(columns = column)\n",
    "    new_test = pd.get_dummies(new_test)\n",
    "    scalar = StandardScaler()\n",
    "    scalar.fit(new_test)\n",
    "    test_input = scalar.transform(new_test)\n",
    "    return test_input\n",
    "\n",
    "\n",
    "def test_data_with_target_cate_preprocess(test, train, output_label=\"exceeds50K\"):\n",
    "    new_test = test.copy()\n",
    "    scalar = StandardScaler()\n",
    "    continuous_test = test[continuous_features]\n",
    "    scalar.fit(continuous_test)\n",
    "    new_continuous_test = scalar.transform(continuous_test)\n",
    "    category_test = test[category_features]\n",
    "    for column in category_test.columns:\n",
    "        ce_target = ce.TargetEncoder()\n",
    "        ce_target.fit(train[column].values, train[output_label].values)\n",
    "        temp_df = ce_target.transform(test[column].values, y=None)\n",
    "        new_test[column] = temp_df\n",
    "    new_test[continuous_features] = new_continuous_test\n",
    "    return new_test\n",
    "\n",
    "\n",
    "def test_data_with_leaveoneout_cate_preprocess(test, train, output_label=\"exceeds50K\"):\n",
    "    new_test = test.copy()\n",
    "    scalar = StandardScaler()\n",
    "    continuous_test = test[continuous_features]\n",
    "    scalar.fit(continuous_test)\n",
    "    new_continuous_test = scalar.transform(continuous_test)\n",
    "    category_test = test[category_features]\n",
    "    for column in category_test.columns:\n",
    "        ce_leave = ce.LeaveOneOutEncoder() \n",
    "        ce_leave.fit(train[column].values, train[output_label].values)\n",
    "        temp_df = ce_leave.transform(test[column].values, y=None)\n",
    "        new_test[column] = temp_df\n",
    "    new_test[continuous_features] = new_continuous_test\n",
    "    return new_test\n",
    "\n",
    "\n",
    "def test_data_with_sm_target_cate_preprocess(test, train, output_label=\"exceeds50K\", smooth=10):\n",
    "    new_test = test.copy()\n",
    "    scalar = StandardScaler()\n",
    "    continuous_test = test[continuous_features]\n",
    "    scalar.fit(continuous_test)\n",
    "    new_continuous_test = scalar.transform(continuous_test)\n",
    "    category_test = test[category_features]\n",
    "    for column in category_test.columns:\n",
    "        ce_target = ce.TargetEncoder(smoothing=smooth)\n",
    "        ce_target.fit(train[column].values, train[output_label].values)\n",
    "        temp_df = ce_target.transform(test[column].values, y=None)\n",
    "        new_test[column] = temp_df\n",
    "    new_test[continuous_features] = new_continuous_test\n",
    "    return new_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7  columns have been detected!\n"
     ]
    }
   ],
   "source": [
    "# preprocess to test\n",
    "test_input = preprocess_new_catogory(ori_demo, ori_test_input) \n",
    "test_input = test_data_simple_without_normalization_preprocess(test_input)\n",
    "# test_input = test_data_with_sm_target_cate_preprocess(test_input, ori_demo, output_label=\"exceeds50K\",smooth=1)\n",
    "# print(test_input.columns)\n",
    "# print(test_input.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       age  fnlwgt  education-num  capital-gain  capital-loss  hours-per-week  \\\n",
      "0       23   32732             10             0             0              25   \n",
      "1       69  165017              9          2538             0              40   \n",
      "2       27   36440             13             0             0              40   \n",
      "3       40  182217             10             0             0              40   \n",
      "4       24   89347              7             0             0              40   \n",
      "...    ...     ...            ...           ...           ...             ...   \n",
      "24416   26  109186             10             0             0              50   \n",
      "24417   52  254680              9             0             0              99   \n",
      "24418   40  116218             10             0             0              45   \n",
      "24419   29  253262             10             0             0              40   \n",
      "24420   46  177536             10             0             0              60   \n",
      "\n",
      "       workclass_ ?  workclass_ Federal-gov  workclass_ Local-gov  \\\n",
      "0                 0                       0                     0   \n",
      "1                 0                       0                     0   \n",
      "2                 0                       0                     0   \n",
      "3                 0                       0                     0   \n",
      "4                 0                       0                     0   \n",
      "...             ...                     ...                   ...   \n",
      "24416             0                       0                     0   \n",
      "24417             0                       0                     0   \n",
      "24418             0                       0                     0   \n",
      "24419             0                       0                     0   \n",
      "24420             0                       0                     0   \n",
      "\n",
      "       workclass_ Never-worked  ...  native-country_ Portugal  \\\n",
      "0                            0  ...                         0   \n",
      "1                            0  ...                         0   \n",
      "2                            0  ...                         0   \n",
      "3                            0  ...                         0   \n",
      "4                            0  ...                         0   \n",
      "...                        ...  ...                       ...   \n",
      "24416                        0  ...                         0   \n",
      "24417                        0  ...                         0   \n",
      "24418                        0  ...                         0   \n",
      "24419                        0  ...                         0   \n",
      "24420                        0  ...                         0   \n",
      "\n",
      "       native-country_ Puerto-Rico  native-country_ Scotland  \\\n",
      "0                                0                         0   \n",
      "1                                0                         0   \n",
      "2                                0                         0   \n",
      "3                                0                         1   \n",
      "4                                0                         0   \n",
      "...                            ...                       ...   \n",
      "24416                            0                         0   \n",
      "24417                            0                         0   \n",
      "24418                            0                         0   \n",
      "24419                            0                         0   \n",
      "24420                            0                         0   \n",
      "\n",
      "       native-country_ South  native-country_ Taiwan  \\\n",
      "0                          0                       0   \n",
      "1                          0                       0   \n",
      "2                          0                       0   \n",
      "3                          0                       0   \n",
      "4                          0                       0   \n",
      "...                      ...                     ...   \n",
      "24416                      0                       0   \n",
      "24417                      0                       0   \n",
      "24418                      0                       0   \n",
      "24419                      0                       0   \n",
      "24420                      0                       0   \n",
      "\n",
      "       native-country_ Thailand  native-country_ Trinadad&Tobago  \\\n",
      "0                             0                                0   \n",
      "1                             0                                0   \n",
      "2                             0                                0   \n",
      "3                             0                                0   \n",
      "4                             0                                0   \n",
      "...                         ...                              ...   \n",
      "24416                         0                                0   \n",
      "24417                         0                                0   \n",
      "24418                         0                                0   \n",
      "24419                         0                                0   \n",
      "24420                         0                                0   \n",
      "\n",
      "       native-country_ United-States  native-country_ Vietnam  \\\n",
      "0                                  1                        0   \n",
      "1                                  1                        0   \n",
      "2                                  1                        0   \n",
      "3                                  0                        0   \n",
      "4                                  1                        0   \n",
      "...                              ...                      ...   \n",
      "24416                              0                        0   \n",
      "24417                              1                        0   \n",
      "24418                              1                        0   \n",
      "24419                              1                        0   \n",
      "24420                              1                        0   \n",
      "\n",
      "       native-country_ Yugoslavia  \n",
      "0                               0  \n",
      "1                               0  \n",
      "2                               0  \n",
      "3                               0  \n",
      "4                               0  \n",
      "...                           ...  \n",
      "24416                           0  \n",
      "24417                           0  \n",
      "24418                           0  \n",
      "24419                           0  \n",
      "24420                           0  \n",
      "\n",
      "[24421 rows x 102 columns]\n"
     ]
    }
   ],
   "source": [
    "# generate test prediction\n",
    "demo_over = over_sampling(demo, \"exceeds50K\")\n",
    "demo_under = under_sampling(demo, \"exceeds50K\")\n",
    "\n",
    "clf_RF = RandomForestClassifier()\n",
    "clf_RF.fit(demo_over.drop(columns=\"exceeds50K\"), demo_over[\"exceeds50K\"])\n",
    "\n",
    "# clf_GBM = GradientBoostingClassifier()\n",
    "# clf_GBM.fit(demo_target_over.drop(columns=\"exceeds50K\"),demo_leave_over[\"exceeds50K\"])\n",
    "\n",
    "print(test_input)\n",
    "\n",
    "test_y_pred = clf_RF.predict(test_input)\n",
    "result = pd.DataFrame(data=test_y_pred, columns=[\"prediction\"])\n",
    "result[\"id\"] = range(1, result.shape[0] + 1)\n",
    "result.to_csv(r\"RF_replaceQ_onehot_submission_new.csv\",index=False)\n",
    "\n",
    "# test_y_pred = clf_GBM.predict(test_input)\n",
    "# result = pd.DataFrame(data=test_y_pred, columns=[\"prediction\"])\n",
    "# result[\"id\"] = range(1, result.shape[0] + 1)\n",
    "# result.to_csv(\"GBM_leave_over_sampling_submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def stacking(clfs, train_data, final_clf,output_label,test):\n",
    "    num_clfs = len(clfs)\n",
    "    data_size = train_data.shape[0] // num_clfs\n",
    "    \n",
    "#     print(\"train_data[output_label] before\",train_data[output_label])\n",
    "    temp_inputs = []\n",
    "    for (clf,i) in zip(clfs, range(len(clfs))):\n",
    "        cur_train_data = train_data.sample(data_size, replace=True)\n",
    "        X = cur_train_data.drop(columns = output_label)\n",
    "        y = cur_train_data[output_label]\n",
    "        print(\"X,y shape\",X.shape,y.shape)\n",
    "        clf.fit(X,y)\n",
    "        temp_input_data = clf.predict(train_data.drop(columns=output_label))\n",
    "        temp_input = pd.DataFrame(data=temp_input_data, columns=[\"clf\" + str(i)])\n",
    "        temp_inputs.append(temp_input)\n",
    "#         print(\"temp_inputs\", temp_inputs)\n",
    "    temp_input = pd.concat(temp_inputs,axis=1)\n",
    "    temp_input.reset_index(drop=True, inplace=True)\n",
    "    train_data.reset_index(drop=True, inplace=True)\n",
    "#     print(\"temp_input\",temp_input)\n",
    "#     print(\"train_data[output_label]\",train_data[output_label])\n",
    "#     assert 1==0\n",
    "    temp_input[output_label] = train_data[output_label]\n",
    "#     print(temp_input)\n",
    "#     temp_input[output_label] = train_data[output_label]\n",
    "    final_clf.fit(temp_input.drop(columns=output_label), temp_input[output_label].data)\n",
    "    \n",
    "    temp_tests = []\n",
    "    for (clf,i) in zip(clfs, range(len(clfs))):\n",
    "        temp_test = pd.DataFrame(data=clf.predict(test), columns=[\"clf\" + str(i)])\n",
    "        temp_tests.append(temp_test)\n",
    "    temp_test = pd.concat(temp_tests,axis=1)\n",
    "    return final_clf.predict(temp_test)\n",
    "\n",
    "\n",
    "# stacking \n",
    "clf_RF_1 = RandomForestClassifier()\n",
    "clf_RF_2 = RandomForestClassifier()\n",
    "clf_RF_3 = RandomForestClassifier()\n",
    "clf_RF_4 = RandomForestClassifier()\n",
    "y_pred = stacking([clf_RF_1, clf_RF_2, clf_RF_3], demo, clf_RF_4, \"exceeds50K\",test_input )\n",
    "result = pd.DataFrame(data=test_y_pred, columns=[\"prediction\"])\n",
    "result[\"id\"] = range(1, result.shape[0] + 1)\n",
    "result.to_csv(\"Stacking(RF,RF,RF)RF_onehot_submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# votingclassifier\n",
    "ori_demo = pd.read_csv(\"../data/train.csv\")\n",
    "ori_test_input = pd.read_csv(\"../data/test.csv\")\n",
    "def basic_preprocess(data):\n",
    "    for column in category_features:\n",
    "        mean_value = data[column].value_counts().index[0]\n",
    "#         print(\"mean\", mean_value)\n",
    "        data[column] = data[column].replace(regex=r'^.*\\?.*$',value=mean_value)\n",
    "#         print(\"column name:\",column,\", all values:\", data[column].unique())\n",
    "    return data\n",
    "ori_demo  = basic_preprocess(ori_demo)\n",
    "X = ori_demo.drop(columns=\"exceeds50K\")\n",
    "y = ori_demo[\"exceeds50K\"]\n",
    "test_input = basic_preprocess(ori_test_input)\n",
    "test_input = preprocess_new_catogory(train=ori_demo, test = test_input)\n",
    "test_input = pd.get_dummies(test_input)\n",
    "# X = basic_preprocess(X)\n",
    "X = pd.get_dummies(X)\n",
    "print(X.shape)\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1,n_estimators=20)\n",
    "clf3 = GaussianNB()\n",
    "clf4 = RandomForestClassifier(random_state=1, n_estimators=21)\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3),('rf2', clf4)], voting='hard')  # 无权重投票\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=10)\n",
    "eclf.fit(x_train,y_train)\n",
    "y_pred = eclf.predict(x_test) \n",
    "f1 = round(f1_score(y_test, y_pred, average='weighted') * 100, 2)\n",
    "acc = round(accuracy_score(y_test, y_pred) * 100, 2)\n",
    "print(\"f1\",f1,\"  acc\",acc)\n",
    "eclf.fit(X,y)\n",
    "test_y_pred = eclf.predict(test_input)\n",
    "result = pd.DataFrame(data=test_y_pred, columns=[\"prediction\"])\n",
    "result[\"id\"] = range(1, result.shape[0] + 1)\n",
    "result.to_csv(r\"RF_replaceQ_onehot_votingClf_submission.csv\",index=False)\n",
    "# 配合网格搜索\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {'lr__C': [1.0, 100.0], 'rf__n_estimators': [5, 200],}  # 搜索寻找最优的lr模型中的C参数和rf模型中的n_estimators\n",
    "grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\n",
    "grid = grid.fit(X, y)\n",
    "print('最优参数：',grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
